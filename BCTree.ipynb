{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BCTree_Debug.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "kOHLzD3SEVtz",
        "pax-ncbxQg6L",
        "RjAjlnW0Iijg",
        "jGCXrpnADywP",
        "-49_yrZInieX",
        "hPx34ziV0uDg",
        "91xhOkyDF7_u"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2Zddx3NIKh-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "22d74bd7-a85c-456d-dc15-37ba321c28be"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niT1p344PP5p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "90ffc588-54c8-4c1a-d7e3-4672e809d515"
      },
      "source": [
        "import json\n",
        "import glob2\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import regex as re\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch import optim\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "!pip install transformers\n",
        "from transformers import BertTokenizer   \n",
        "from transformers import BertModel"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
            "\u001b[K     |████████████████████████████████| 675kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 16.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 17.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 38.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=48f905c24f8cfb11c0c941878b182852d3f6852725a5d8dca5d72b3fc032c2a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHq-fvZYRiA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Node():\n",
        "  def __init__(self,x=None,y=None,id=None,node1=None,node2=None):\n",
        "    global gb\n",
        "    if(id is None and (node1 is None or node2 is None)):\n",
        "      raise Exception(\"To create non virtual nodes, id is required\")\n",
        "    if(id is not None):\n",
        "      self.id=str(id)\n",
        "      self.is_leaf_node=True\n",
        "    else:\n",
        "      gb.varint+=1\n",
        "      self.id='vir_'+ str(gb.varint)\n",
        "      self.node1=node1\n",
        "      self.node2=node2  \n",
        "      if('vir_' in node1.id and 'vir_' in node2.id):\n",
        "        self.is_stance_node=False\n",
        "      else:\n",
        "        self.is_stance_node=True\n",
        "      self.is_leaf_node=False  \n",
        "    self.x=x\n",
        "    self.y=y\n",
        "    self.c=None\n",
        "    self.h=None    "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dLnrQE-THLw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Global:\n",
        "    def __init__(self):\n",
        "      self.varint=0\n",
        "      self.g_parent=None\n",
        "      self.g_nodes=[] \n",
        "    def reset(self):\n",
        "      self.varint=0\n",
        "      self.g_parent=None\n",
        "      self.g_nodes=[]\n",
        "\n",
        "def get_key_value(dicts,search_id,keys=None):\n",
        "  res={}\n",
        "  for d in dicts:\n",
        "    if('id' in d.keys()):\n",
        "      if(str(d['id'])==str(search_id)):\n",
        "        for k in keys:\n",
        "          res[k]=d[k]\n",
        "        break\n",
        "    else:\n",
        "      if(search_id in d.keys()):  \n",
        "        return d[search_id]\n",
        "      else:\n",
        "        return None        \n",
        "  return list(res.values()) \n",
        "\n",
        "def uniform_embed(size):\n",
        "  return np.random.uniform(low=-0.5, high=0.5, size=(size))\n",
        "\n",
        "def create_tree(tree,tweets=None,stance_dict=None,parent=None,embed_size=2400):\n",
        "  global gb\n",
        "  children=[]\n",
        "  for key,value in tree.items():\n",
        "    if(type(value)==dict):      \n",
        "      child=create_tree(value,tweets,stance_dict,parent=key,embed_size=embed_size)\n",
        "    else:\n",
        "      embed=get_key_value(dicts=tweets,search_id=key,keys=['text'])\n",
        "      if(len(embed)>0 and len(embed[0])>0):\n",
        "        embed=embed[0]\n",
        "      else:\n",
        "        embed=uniform_embed(embed_size)            \n",
        "      child=Node(id=key,x=embed)\n",
        "    children.append(child) \n",
        "  if(gb.g_parent is not None):\n",
        "    arr=[gb.g_parent]\n",
        "    arr.extend(children)\n",
        "    children=arr \n",
        "  if(parent is not None):\n",
        "    nodes=[]\n",
        "    for child in children:\n",
        "      if('vir_' not in str(parent) and 'vir_' in child.id):\n",
        "        nodes.append(child)\n",
        "        continue\n",
        "      embed=get_key_value(dicts=tweets,search_id=parent,keys=['text']) \n",
        "      if(len(embed)>0 and len(embed[0])>0):\n",
        "        embed=embed[0]  \n",
        "      else:\n",
        "        embed=uniform_embed(embed_size)      \n",
        "      parent_node=Node(id=parent,x=embed) \n",
        "      stance=None\n",
        "      if('var_' not in parent_node.id and 'var_' not in child.id):\n",
        "        stance=get_key_value(dicts=[stance_dict],search_id=child.id) \n",
        "      node=Node(node1=parent_node,node2=child,y=stance)\n",
        "      # print(parent,child.id,node.id)\n",
        "      gb.g_nodes.extend([parent_node,child,node])\n",
        "      nodes.append(node)\n",
        "    node1=nodes[0] \n",
        "    for node_iter in range(1,len(nodes)):\n",
        "      node2=nodes[node_iter]\n",
        "      node=Node(node1=node1,node2=node2)\n",
        "      # print(node1.id,node2.id,node.id)\n",
        "      gb.g_nodes.extend([node])\n",
        "      node1=node \n",
        "    embed=get_key_value(dicts=tweets,search_id=parent,keys=['text']) \n",
        "    if(len(embed)>0 and len(embed[0])>0):\n",
        "      embed=embed[0]  \n",
        "    else:\n",
        "      embed=uniform_embed(embed_size)            \n",
        "    gb.g_parent=Node(id=parent,x=embed)\n",
        "    return node1\n",
        "  return children     "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOHLzD3SEVtz",
        "colab_type": "text"
      },
      "source": [
        "# SKP Encoder python2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IV-_CED6upUP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f00f1f7b-adff-4294-94c1-bd6fa48014d5"
      },
      "source": [
        "# !git clone https://github.com/ryankiros/skip-thoughts.git\n",
        "!wget http://www.cs.toronto.edu/~rkiros/models/dictionary.txt -P skip-thoughts\n",
        "!wget http://www.cs.toronto.edu/~rkiros/models/utable.npy -P skip-thoughts\n",
        "!wget http://www.cs.toronto.edu/~rkiros/models/btable.npy -P skip-thoughts\n",
        "!wget http://www.cs.toronto.edu/~rkiros/models/uni_skip.npz -P skip-thoughts\n",
        "!wget http://www.cs.toronto.edu/~rkiros/models/uni_skip.npz.pkl -P skip-thoughts\n",
        "!wget http://www.cs.toronto.edu/~rkiros/models/bi_skip.npz -P skip-thoughts\n",
        "!wget http://www.cs.toronto.edu/~rkiros/models/bi_skip.npz.pkl -P skip-thoughts"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-21 03:49:17--  http://www.cs.toronto.edu/~rkiros/models/dictionary.txt\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7996547 (7.6M) [text/plain]\n",
            "Saving to: ‘skip-thoughts/dictionary.txt’\n",
            "\n",
            "dictionary.txt      100%[===================>]   7.63M  11.0MB/s    in 0.7s    \n",
            "\n",
            "2020-06-21 03:49:17 (11.0 MB/s) - ‘skip-thoughts/dictionary.txt’ saved [7996547/7996547]\n",
            "\n",
            "--2020-06-21 03:49:19--  http://www.cs.toronto.edu/~rkiros/models/utable.npy\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2342138474 (2.2G)\n",
            "Saving to: ‘skip-thoughts/utable.npy’\n",
            "\n",
            "utable.npy          100%[===================>]   2.18G  84.4MB/s    in 28s     \n",
            "\n",
            "2020-06-21 03:49:47 (79.0 MB/s) - ‘skip-thoughts/utable.npy’ saved [2342138474/2342138474]\n",
            "\n",
            "--2020-06-21 03:49:49--  http://www.cs.toronto.edu/~rkiros/models/btable.npy\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2342138474 (2.2G)\n",
            "Saving to: ‘skip-thoughts/btable.npy’\n",
            "\n",
            "btable.npy          100%[===================>]   2.18G  33.6MB/s    in 31s     \n",
            "\n",
            "2020-06-21 03:50:20 (71.1 MB/s) - ‘skip-thoughts/btable.npy’ saved [2342138474/2342138474]\n",
            "\n",
            "--2020-06-21 03:50:22--  http://www.cs.toronto.edu/~rkiros/models/uni_skip.npz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 663989216 (633M)\n",
            "Saving to: ‘skip-thoughts/uni_skip.npz’\n",
            "\n",
            "uni_skip.npz        100%[===================>] 633.23M  77.9MB/s    in 8.3s    \n",
            "\n",
            "2020-06-21 03:50:30 (76.5 MB/s) - ‘skip-thoughts/uni_skip.npz’ saved [663989216/663989216]\n",
            "\n",
            "--2020-06-21 03:50:31--  http://www.cs.toronto.edu/~rkiros/models/uni_skip.npz.pkl\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 693\n",
            "Saving to: ‘skip-thoughts/uni_skip.npz.pkl’\n",
            "\n",
            "uni_skip.npz.pkl    100%[===================>]     693  --.-KB/s    in 0s      \n",
            "\n",
            "2020-06-21 03:50:32 (90.3 MB/s) - ‘skip-thoughts/uni_skip.npz.pkl’ saved [693/693]\n",
            "\n",
            "--2020-06-21 03:50:34--  http://www.cs.toronto.edu/~rkiros/models/bi_skip.npz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 289340074 (276M)\n",
            "Saving to: ‘skip-thoughts/bi_skip.npz’\n",
            "\n",
            "bi_skip.npz         100%[===================>] 275.94M  70.2MB/s    in 4.1s    \n",
            "\n",
            "2020-06-21 03:50:38 (66.5 MB/s) - ‘skip-thoughts/bi_skip.npz’ saved [289340074/289340074]\n",
            "\n",
            "--2020-06-21 03:50:40--  http://www.cs.toronto.edu/~rkiros/models/bi_skip.npz.pkl\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 689\n",
            "Saving to: ‘skip-thoughts/bi_skip.npz.pkl’\n",
            "\n",
            "bi_skip.npz.pkl     100%[===================>]     689  --.-KB/s    in 0s      \n",
            "\n",
            "2020-06-21 03:50:40 (105 MB/s) - ‘skip-thoughts/bi_skip.npz.pkl’ saved [689/689]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlS81VnUzY_z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "cee25d3d-5fbd-4aa4-ed99-3d375f98907a"
      },
      "source": [
        "# %cd skip-thoughts\n",
        "# import importlib\n",
        "# importlib.reload(skipthoughts)\n",
        "import skipthoughts\n",
        "model = skipthoughts.load_model()\n",
        "encoder = skipthoughts.Encoder(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'skip-thoughts'\n",
            "/content/skip-thoughts\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING (theano.tensor.blas): We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pax-ncbxQg6L",
        "colab_type": "text"
      },
      "source": [
        "# SKP encoder python3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y78n9GkwQlW-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "516ddb79-ffc2-42df-99eb-3a53f340fac2"
      },
      "source": [
        "!wget \"http://download.tensorflow.org/models/skip_thoughts_bi_2017_02_16.tar.gz\"\n",
        "!tar -xvf skip_thoughts_bi_2017_02_16.tar.gz\n",
        "!rm skip_thoughts_bi_2017_02_16.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-21 06:02:01--  http://download.tensorflow.org/models/skip_thoughts_bi_2017_02_16.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 173.194.214.128, 2607:f8b0:400c:c0b::80\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|173.194.214.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5680270817 (5.3G) [application/x-gzip]\n",
            "Saving to: ‘skip_thoughts_bi_2017_02_16.tar.gz’\n",
            "\n",
            "skip_thoughts_bi_20 100%[===================>]   5.29G  58.6MB/s    in 74s     \n",
            "\n",
            "2020-06-21 06:03:15 (72.9 MB/s) - ‘skip_thoughts_bi_2017_02_16.tar.gz’ saved [5680270817/5680270817]\n",
            "\n",
            "skip_thoughts_bi_2017_02_16/\n",
            "skip_thoughts_bi_2017_02_16/model.ckpt-500008.data-00000-of-00001\n",
            "skip_thoughts_bi_2017_02_16/vocab.txt\n",
            "skip_thoughts_bi_2017_02_16/model.ckpt-500008.index\n",
            "skip_thoughts_bi_2017_02_16/embeddings.npy\n",
            "skip_thoughts_bi_2017_02_16/model.ckpt-500008.meta\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AaMrmVASYMK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "39dc2b9d-037e-40db-f5e0-7bdbc4b943aa"
      },
      "source": [
        "!git clone https://github.com/elvisyjlin/skip-thoughts.git\n",
        "%tensorflow_version 1.x\n",
        "%cd skip-thoughts/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'skip-thoughts'...\n",
            "remote: Enumerating objects: 35, done.\u001b[K\n",
            "Unpacking objects:   2% (1/35)   \rUnpacking objects:   5% (2/35)   \rUnpacking objects:   8% (3/35)   \rUnpacking objects:  11% (4/35)   \rUnpacking objects:  14% (5/35)   \rUnpacking objects:  17% (6/35)   \rUnpacking objects:  20% (7/35)   \rUnpacking objects:  22% (8/35)   \rUnpacking objects:  25% (9/35)   \rUnpacking objects:  28% (10/35)   \rUnpacking objects:  31% (11/35)   \rUnpacking objects:  34% (12/35)   \rUnpacking objects:  37% (13/35)   \rUnpacking objects:  40% (14/35)   \rUnpacking objects:  42% (15/35)   \rUnpacking objects:  45% (16/35)   \rUnpacking objects:  48% (17/35)   \rUnpacking objects:  51% (18/35)   \rremote: Total 35 (delta 0), reused 0 (delta 0), pack-reused 35\u001b[K\n",
            "Unpacking objects:  54% (19/35)   \rUnpacking objects:  57% (20/35)   \rUnpacking objects:  60% (21/35)   \rUnpacking objects:  62% (22/35)   \rUnpacking objects:  65% (23/35)   \rUnpacking objects:  68% (24/35)   \rUnpacking objects:  71% (25/35)   \rUnpacking objects:  74% (26/35)   \rUnpacking objects:  77% (27/35)   \rUnpacking objects:  80% (28/35)   \rUnpacking objects:  82% (29/35)   \rUnpacking objects:  85% (30/35)   \rUnpacking objects:  88% (31/35)   \rUnpacking objects:  91% (32/35)   \rUnpacking objects:  94% (33/35)   \rUnpacking objects:  97% (34/35)   \rUnpacking objects: 100% (35/35)   \rUnpacking objects: 100% (35/35), done.\n",
            "TensorFlow 1.x selected.\n",
            "/content/skip-thoughts\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dM1OqcURRRF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        },
        "outputId": "9f0da27e-3a3d-42ed-d827-594f7a402d34"
      },
      "source": [
        "from skip_thoughts import configuration\n",
        "from skip_thoughts import encoder_manager\n",
        "\n",
        "VOCAB_FILE = \"skip_thoughts_bi_2017_02_16/vocab.txt\"\n",
        "EMBEDDING_MATRIX_FILE = \"skip_thoughts_bi_2017_02_16/embeddings.npy\"\n",
        "CHECKPOINT_PATH = \"skip_thoughts_bi_2017_02_16/model.ckpt-500008\"\n",
        "\n",
        "encoder = encoder_manager.EncoderManager()\n",
        "encoder.load_model(configuration.model_config(bidirectional_encoder=True),\n",
        "                   vocabulary_file=VOCAB_FILE,\n",
        "                   embedding_matrix_file=EMBEDDING_MATRIX_FILE,\n",
        "                   checkpoint_path=CHECKPOINT_PATH)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /content/skip-thoughts/skip_thoughts/encoder_manager.py:63: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "INFO:tensorflow:Reading vocabulary from skip_thoughts_bi_2017_02_16/vocab.txt\n",
            "WARNING:tensorflow:From /content/skip-thoughts/skip_thoughts/encoder_manager.py:64: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "INFO:tensorflow:Loaded vocabulary with 930914 words.\n",
            "INFO:tensorflow:Loading embedding matrix from skip_thoughts_bi_2017_02_16/embeddings.npy\n",
            "INFO:tensorflow:Loaded embedding matrix with shape (930914, 620)\n",
            "INFO:tensorflow:Building model.\n",
            "WARNING:tensorflow:From /content/skip-thoughts/skip_thoughts/skip_thoughts_model.py:71: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\n",
            "WARNING:tensorflow:From /content/skip-thoughts/skip_thoughts/skip_thoughts_model.py:127: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/skip-thoughts/skip_thoughts/skip_thoughts_model.py:230: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/skip-thoughts/skip_thoughts/skip_thoughts_model.py:231: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/skip-thoughts/skip_thoughts/skip_thoughts_model.py:246: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /content/skip-thoughts/skip_thoughts/ops/gru_cell.py:117: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/skip-thoughts/skip_thoughts/skip_thoughts_model.py:47: The name tf.svd is deprecated. Please use tf.linalg.svd instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/skip-thoughts/skip_thoughts/skip_thoughts_model.py:47: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/skip-thoughts/skip_thoughts/skip_thoughts_model.py:360: The name tf.train.create_global_step is deprecated. Please use tf.compat.v1.train.create_global_step instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/skip-thoughts/skip_thoughts/skip_thoughts_encoder.py:151: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/skip-thoughts/skip_thoughts/skip_thoughts_encoder.py:122: The name tf.gfile.IsDirectory is deprecated. Please use tf.io.gfile.isdir instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/skip-thoughts/skip_thoughts/encoder_manager.py:85: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/skip-thoughts/skip_thoughts/encoder_manager.py:87: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "INFO:tensorflow:Loading model from checkpoint: skip_thoughts_bi_2017_02_16/model.ckpt-500008\n",
            "INFO:tensorflow:Restoring parameters from skip_thoughts_bi_2017_02_16/model.ckpt-500008\n",
            "INFO:tensorflow:Successfully loaded checkpoint: model.ckpt-500008\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjAjlnW0Iijg",
        "colab_type": "text"
      },
      "source": [
        "# DeepMoji Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z59XYZIqauxv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        },
        "outputId": "7a3c4aa4-4d04-42da-86dd-7370d8686d2e"
      },
      "source": [
        "# for deepMoji\n",
        "!git clone https://github.com/zzsza/DeepMoji-Python3.git\n",
        "%cd DeepMoji-Python3/DeepMoji-master\n",
        "!python scripts/download_weights.py\n",
        "!pip install emoji\n",
        "# replace downloaded encode_texts.py file with modified file"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'DeepMoji-Python3'...\n",
            "remote: Enumerating objects: 85, done.\u001b[K\n",
            "remote: Total 85 (delta 0), reused 0 (delta 0), pack-reused 85\u001b[K\n",
            "Unpacking objects: 100% (85/85), done.\n",
            "/content/DeepMoji-Python3/DeepMoji-master\n",
            "About to download the pretrained weights file from https://www.dropbox.com/s/xqarafsl6a8f9ny/deepmoji_weights.hdf5?dl=0#\n",
            "The size of the file is roughly 85MB. Continue? [y/n]\n",
            "y\n",
            "Downloading...\n",
            "Running system call: wget https://www.dropbox.com/s/xqarafsl6a8f9ny/deepmoji_weights.hdf5?dl=0# -O /content/DeepMoji-Python3/DeepMoji-master/model/deepmoji_weights.hdf5\n",
            "--2020-06-22 07:25:53--  https://www.dropbox.com/s/xqarafsl6a8f9ny/deepmoji_weights.hdf5?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.67.1, 2620:100:6023:1::a27d:4301\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.67.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/xqarafsl6a8f9ny/deepmoji_weights.hdf5 [following]\n",
            "--2020-06-22 07:25:53--  https://www.dropbox.com/s/raw/xqarafsl6a8f9ny/deepmoji_weights.hdf5\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucf60450f87130cd2deb6575e0a8.dl.dropboxusercontent.com/cd/0/inline/A6E8CZ7Elc2iI4VS2Kbt3APmP-kFYxQSA2zc9e5A5wnrKheWKduSoIvRLbaymId_nDlnqAwVq-rWB3QqqG7OG_SYczSBQCW2YL1LlsbtJ2jJ4Q/file# [following]\n",
            "--2020-06-22 07:25:54--  https://ucf60450f87130cd2deb6575e0a8.dl.dropboxusercontent.com/cd/0/inline/A6E8CZ7Elc2iI4VS2Kbt3APmP-kFYxQSA2zc9e5A5wnrKheWKduSoIvRLbaymId_nDlnqAwVq-rWB3QqqG7OG_SYczSBQCW2YL1LlsbtJ2jJ4Q/file\n",
            "Resolving ucf60450f87130cd2deb6575e0a8.dl.dropboxusercontent.com (ucf60450f87130cd2deb6575e0a8.dl.dropboxusercontent.com)... 162.125.67.15, 2620:100:6023:15::a27d:430f\n",
            "Connecting to ucf60450f87130cd2deb6575e0a8.dl.dropboxusercontent.com (ucf60450f87130cd2deb6575e0a8.dl.dropboxusercontent.com)|162.125.67.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/A6EQ5qadDs7AjIpzfoLtMUeRY9tPqaJ7gXzemmUYcn5NOAeaq5V2CT9mL7vnVvnr6PlKuAqWIIgIj3YDWlVYZhR9PvTWJ7yBUDx-p22jNz-VwguISE3XC8hesXIvUkeoiGma0f2L-NrE5ZlazCp_w4lf3xL7BHMlIA9Il0aG0cLLF-QyY-570TBOg0maefVDs5I1Nmpd7-D0kc4W40DrMGDCb81Pxo6N5J9WqY7zWZJLbF-yTtJb-rAT7Eeyxr0rmXT63FOEd4Usml-gwu79pa1nZhw6IdRm0Uy82elz1ccB91KdEhGWMoEgMenUINH0g4P8_U2mlyrTrUurBAnYgbKc/file [following]\n",
            "--2020-06-22 07:25:55--  https://ucf60450f87130cd2deb6575e0a8.dl.dropboxusercontent.com/cd/0/inline2/A6EQ5qadDs7AjIpzfoLtMUeRY9tPqaJ7gXzemmUYcn5NOAeaq5V2CT9mL7vnVvnr6PlKuAqWIIgIj3YDWlVYZhR9PvTWJ7yBUDx-p22jNz-VwguISE3XC8hesXIvUkeoiGma0f2L-NrE5ZlazCp_w4lf3xL7BHMlIA9Il0aG0cLLF-QyY-570TBOg0maefVDs5I1Nmpd7-D0kc4W40DrMGDCb81Pxo6N5J9WqY7zWZJLbF-yTtJb-rAT7Eeyxr0rmXT63FOEd4Usml-gwu79pa1nZhw6IdRm0Uy82elz1ccB91KdEhGWMoEgMenUINH0g4P8_U2mlyrTrUurBAnYgbKc/file\n",
            "Reusing existing connection to ucf60450f87130cd2deb6575e0a8.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 89610904 (85M) [application/octet-stream]\n",
            "Saving to: ‘/content/DeepMoji-Python3/DeepMoji-master/model/deepmoji_weights.hdf5’\n",
            "\n",
            "/content/DeepMoji-P 100%[===================>]  85.46M  25.5MB/s    in 3.4s    \n",
            "\n",
            "2020-06-22 07:25:59 (25.5 MB/s) - ‘/content/DeepMoji-Python3/DeepMoji-master/model/deepmoji_weights.hdf5’ saved [89610904/89610904]\n",
            "\n",
            "Downloaded weights to model/deepmoji_weights.hdf5\n",
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/8d/521be7f0091fe0f2ae690cc044faf43e3445e0ff33c574eae752dd7e39fa/emoji-0.5.4.tar.gz (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-0.5.4-cp36-none-any.whl size=42176 sha256=196862db73a3c5421d0592e81bdf1793d55d3c33aceca293938d27076f9fc86a\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/a9/0a/4f8e8cce8074232aba240caca3fade315bb49fac68808d1a9c\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-0.5.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nB-T5Kc8UrH3",
        "colab_type": "text"
      },
      "source": [
        "# BERT Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmFHWKvsUlet",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SentenceClassifier(nn.Module):\n",
        "  def __init__(self):\n",
        "      super(SentenceClassifier, self).__init__()\n",
        "      self.bert_layer = BertModel.from_pretrained('/content/drive/My Drive/BERT_CONFIG')\n",
        "      self.linear = nn.Linear(768*2, 3)\n",
        "      self.softmax = nn.Softmax()\n",
        "\n",
        "  def forward(self,seq,mask):\n",
        "      _,cls = self.bert_layer(seq, attention_mask = mask)\n",
        "      return cls.tolist()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtCTCPODEdKd",
        "colab_type": "text"
      },
      "source": [
        "# Encoders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So06O3J_cKNd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import examples.encode_texts\n",
        "\n",
        "class GLOVE:\n",
        "  def __init__(self,embedding_file):\n",
        "    self.vocab={}\n",
        "    self.unk=[0 for i in range(300)]\n",
        "    with open(embedding_file, 'rt', encoding='utf-8') as f:\n",
        "      for line in f:\n",
        "        splitline=line.split()\n",
        "        self.vocab[splitline[0]]=[float(value) for value in splitline[1:]]\n",
        "    self.size=300    \n",
        "  def embed(self,tokens):\n",
        "    embed = [self.vocab.get(token,self.unk) for token in tokens]\n",
        "    embed = [sum(x)/len(x) for x in zip(*embed)]\n",
        "    return embed\n",
        "\n",
        "class DeepMoji:\n",
        "  def __init__(self):\n",
        "    self.size=2304\n",
        "  def embed(self,tokens):\n",
        "    embed=examples.encode_texts.encode(np.array([''.join(tokens)]))\n",
        "    return embed.reshape(2304)\n",
        "\n",
        "class SKP:\n",
        "  def __init__(self,encoder):\n",
        "    self.encoder=encoder\n",
        "    self.size=2400\n",
        "  def embed(self,tokens):\n",
        "    embed=self.encoder.encode(tokens) \n",
        "    embed=[sum(x)/len(x) for x in zip(*embed)]\n",
        "    return embed\n",
        "\n",
        "class skpemt:\n",
        "  def __init__(self,encoder):\n",
        "    self.encoder=encoder\n",
        "    self.size=2400+2304\n",
        "  def embed(self,tokens):\n",
        "    encoding=self.encoder.encode(tokens) \n",
        "    encoding=[sum(x)/len(x) for x in zip(*encoding)]\n",
        "    deepmoji_encoding=examples.encode_texts.encode(np.array([''.join(tokens)]))\n",
        "    deepmoji_encoding= list(deepmoji_encoding.reshape(2304))\n",
        "    encoding.extend(deepmoji_encoding)\n",
        "    return encoding \n",
        "\n",
        "class BERT:\n",
        "  def __init__(self):\n",
        "    self.model=SentenceClassifier()\n",
        "    self.model.load_state_dict(torch.load('/content/drive/My Drive/BERT_CONFIG/multinli_model.pt'))\n",
        "    self.size=768\n",
        "    self.tokenizer = BertTokenizer.from_pretrained('/content/drive/My Drive/BERT_CONFIG')    \n",
        "  def tokenize(self,tokens):\n",
        "    return [self.tokenizer.convert_tokens_to_ids('[CLS]')]\\\n",
        "      +[self.tokenizer.convert_tokens_to_ids(tok) for tok in tokens]\n",
        "  def embed(self,tokens):\n",
        "    tokenized=torch.tensor(self.tokenize(tokens)).view(1,-1)\n",
        "    mask=torch.tensor([1 for i in range(tokenized.size(-1))]).view(1,-1)\n",
        "    return self.model(tokenized,mask)            "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A71C2kpIDq7R",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing Input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ShAG5hdwiEd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_datapoint(source,reply_list,struct,stance_dict,embedding):\n",
        "    tweets=reply_list+[source]\n",
        "    create_tree(struct,tweets,stance_dict,embed_size=embedding.size)\n",
        "\n",
        "def preprocess(tweet):\n",
        "    # remove @mentions, RT,MT,DM,PRT,HT,CC, URLs\n",
        "    contractions = { \n",
        "      \"n't\": \"not\",\n",
        "      \"'ve\": \"have\",\n",
        "      \"'d\": \"would\",\n",
        "      \"'ll\": \"will\",\n",
        "      \"'m\": \"am\",\n",
        "      \"ma'am\": \"madam\",\n",
        "      \"'re\": \"they are\"\n",
        "      }\n",
        "    text=tweet.lower()  \n",
        "    tokens=text.split()\n",
        "    # print(tokens)\n",
        "    for indx,token in enumerate(tokens):\n",
        "        for contra in contractions.keys():\n",
        "            if(contra in token):\n",
        "                tokens[indx]=' '.join([token.replace(contra,''),contractions[contra]])\n",
        "    text=' '.join(tokens)\n",
        "    # Format words and remove unwanted characters    \n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\<a href', ' ', text)\n",
        "    text = re.sub(r'([@?])(\\w+)\\b', ' ', text)\n",
        "    text = re.sub(r'&amp;', '', text) \n",
        "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
        "    text = re.sub(r'<br />', ' ', text)\n",
        "    text = re.sub(r'\\'', ' ', text)\n",
        "    text=text.replace('mt ',' ').replace('rt ',' ').replace('dm ',' ').replace('prt ',' ').replace('ht ',' ').replace('cc ',' ')\n",
        "    tokens = text.split()\n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "    tokens = [w for w in tokens if not w in stops]\n",
        "    return tokens"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFONag3ECPeG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "c7dd13b6-0252-47e3-b5eb-2be4f7a5c7f9"
      },
      "source": [
        "text='MT @euronews France: 10 dead after shooting at HQ of satirical weekly #CharlieHebdo. If Zionists/Jews did this they\\'d be nuking Israel'\n",
        "preprocess(text)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['france',\n",
              " '10',\n",
              " 'dead',\n",
              " 'shooting',\n",
              " 'hq',\n",
              " 'satirical',\n",
              " 'weekly',\n",
              " 'charliehebdo',\n",
              " 'zionists',\n",
              " 'jews',\n",
              " 'would',\n",
              " 'nuking',\n",
              " 'israel']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGCXrpnADywP",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GON9ei_6nZf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TreeLSTMCell(nn.Module):\n",
        "  def __init__(self,x_size,h_size):\n",
        "    super(TreeLSTMCell, self).__init__()\n",
        "    self.W_iou = nn.Linear(x_size, 3 * h_size, bias=False)\n",
        "    self.U_iou = nn.Linear(h_size, 3 * h_size, bias=False)\n",
        "    self.b_iou = nn.Parameter(torch.zeros(1, 3 * h_size))\n",
        "    self.U_f = nn.Linear(2 * h_size, 2 * h_size)\n",
        "    self.b_f = nn.Parameter(torch.zeros(1, 2 * h_size))      \n",
        "    self.h=torch.zeros(1,h_size)\n",
        "    self.c=torch.zeros(1,h_size)\n",
        "    self.sigmoid=nn.Sigmoid()\n",
        "    self.tanh=nn.Tanh()\n",
        "    self.conv=nn.Conv2d(in_channels=1,out_channels=1, kernel_size=2, stride=1,padding=(0,1))\n",
        "    self.maxpool=nn.MaxPool2d(kernel_size=(1,2),stride=1)\n",
        "    self.stance_linear=nn.Linear(h_size,4)\n",
        "    self.veracity_linear=nn.Linear(h_size,3)\n",
        "    # self.softmax=nn.LogSoftmax(dim=-1)\n",
        "    self.h_size=h_size\n",
        "  def reset_h_c(self):\n",
        "    self.h=torch.zeros(1,self.h_size)\n",
        "    self.c=torch.zeros(1,self.h_size)    \n",
        "  def forward(self,node,is_last=False):      \n",
        "    if(node.is_leaf_node):\n",
        "      x=torch.tensor(node.x,dtype=torch.float)\n",
        "      x_iou=self.W_iou(x.view(1,-1)) \n",
        "      u_iou=self.U_iou(self.h)\n",
        "      b_iou=self.b_iou      \n",
        "      xs=torch.chunk(x_iou,3,-1)\n",
        "      us=torch.chunk(u_iou,3,-1)\n",
        "      bs=torch.chunk(b_iou,3,-1)       \n",
        "      i=self.sigmoid(xs[0]+us[0]+bs[0])\n",
        "      o=self.sigmoid(xs[1]+us[1]+bs[1])\n",
        "      u=self.sigmoid(xs[2]+us[2]+bs[2])\n",
        "      self.c=i*u\n",
        "      self.h=o*self.tanh(self.c)\n",
        "      node.h=self.h\n",
        "      node.c=self.c\n",
        "    else:\n",
        "      h_stack=torch.stack([node.node1.h,node.node2.h])\n",
        "      u_f=self.U_f(h_stack.view(1,-1))\n",
        "      f=self.sigmoid(u_f+self.b_f)\n",
        "      h_hat=self.maxpool(self.conv(h_stack.permute(1,0,2).unsqueeze(0))).view(1,-1)\n",
        "      u_iou=self.U_iou(h_hat)\n",
        "      us=torch.chunk(u_iou,3,-1)\n",
        "      bs=torch.chunk(self.b_iou,3,-1)            \n",
        "      i=self.sigmoid(us[0]+bs[0])\n",
        "      o=self.sigmoid(us[1]+bs[1])\n",
        "      u=self.sigmoid(us[2]+bs[2])\n",
        "      self.c=i*u + torch.sum(f.view(-1,self.h_size)*torch.stack([node.node1.c,node.node2.c]).view(-1,self.h_size),0)\n",
        "      self.h=o*self.tanh(self.c)\n",
        "      node.h=self.h\n",
        "      node.c=self.c\n",
        "      if(node.is_stance_node):\n",
        "        stance=self.stance_linear(self.h)\n",
        "        return stance        \n",
        "      if(is_last):\n",
        "        veracity=self.veracity_linear(self.h)\n",
        "        return veracity  "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7BUz6Z-D9Im",
        "colab_type": "text"
      },
      "source": [
        "# Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCge3WdLxGVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "veracity_enum = { \n",
        "  'true':0,\n",
        "  'false':1,\n",
        "  'unverified':2\n",
        "}\n",
        "\n",
        "stance_enum={\n",
        "    'agreed':0,\n",
        "    'comment':1,\n",
        "    'appeal-for-more-information':2,\n",
        "    'disagreed':3\n",
        "}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtpE3flOxJjG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Event:\n",
        "  def __init__(self,path):\n",
        "    self.path=path\n",
        "    self.treelist=[]\n",
        "  def create_tree(self,gb):\n",
        "    dirs=os.listdir(self.path)\n",
        "    for d in dirs:\n",
        "      if(d=='.DS_Store'):\n",
        "        continue\n",
        "      source=glob2.glob(self.path+'/'+ d + '/source-tweets/*.json')[0]\n",
        "      replies=glob2.glob(self.path+'/'+ d + '/reactions/*.json')\n",
        "      structure=self.path+'/'+ d + '/structure.json'\n",
        "      gb.reset()\n",
        "      reply_list=[]\n",
        "      with open(source,'r') as f:\n",
        "        source=json.load(f,parse_int=None)\n",
        "      processed_text= preprocess(source['text'])  \n",
        "      if(len(processed_text)>0):  \n",
        "        source={'id':source['id'],'text':embedding.embed(processed_text)}  \n",
        "      else:\n",
        "        source={'id':source['id'],'text':processed_text}        \n",
        "      for reply in replies:\n",
        "        with open(reply,'r') as f:\n",
        "          reply=json.load(f,parse_int=None)\n",
        "        processed_text= preprocess(reply['text'])\n",
        "        if(len(processed_text)>0):\n",
        "          reply={'id':reply['id'],'text':embedding.embed(processed_text)}\n",
        "        else:\n",
        "          reply={'id':reply['id'],'text':processed_text} \n",
        "        reply_list.append(reply)\n",
        "        # print(reply_list)      \n",
        "      with open(structure,'r') as f:\n",
        "        struct=json.load(f,parse_int=None)\n",
        "      create_datapoint(source,reply_list,struct,stance_dict,embedding=embedding)\n",
        "      self.treelist.append((source['id'],gb.g_nodes))      "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyYAVCeTxP-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import ast\n",
        "event_labels=['charliehebdo','sydneysiege','germanwings-crash','ferguson','ottawashooting']\n",
        "# load stance labels\n",
        "# rumour_files=glob2.glob('/content/drive/My Drive/rumoureval-data/traindev/*subtaskA*.json')\n",
        "# stance_dict={}\n",
        "# for file in rumour_files:\n",
        "#   with open(file,'r') as f:\n",
        "#     stance_dict.update(json.load(f,parse_int=None))\n",
        "# for k,v in stance_dict.items():\n",
        "#   stance_dict[k]=stance_enum[v]\n",
        "\n",
        "def convert_annotations(annotation, string = True):\n",
        "    if 'misinformation' in annotation.keys() and 'true'in annotation.keys():\n",
        "        if int(annotation['misinformation'])==0 and int(annotation['true'])==0:\n",
        "            if string:\n",
        "                label = \"unverified\"\n",
        "            else:\n",
        "                label = 2\n",
        "        elif int(annotation['misinformation'])==0 and int(annotation['true'])==1 :\n",
        "            if string:\n",
        "                label = \"true\"\n",
        "            else:\n",
        "                label = 1\n",
        "        elif int(annotation['misinformation'])==1 and int(annotation['true'])==0 :\n",
        "            if string:\n",
        "                label = \"false\"\n",
        "            else:\n",
        "                label = 0\n",
        "        elif int(annotation['misinformation'])==1 and int(annotation['true'])==1:\n",
        "            print (\"OMG! They both are 1!\")\n",
        "            print(annotation['misinformation'])\n",
        "            print(annotation['true'])\n",
        "            label = None\n",
        "            \n",
        "    elif 'misinformation' in annotation.keys() and 'true' not in annotation.keys():\n",
        "        # all instances have misinfo label but don't have true label\n",
        "        if int(annotation['misinformation'])==0:\n",
        "            if string:\n",
        "                label = \"unverified\"\n",
        "            else:\n",
        "                label = 2\n",
        "        elif int(annotation['misinformation'])==1:\n",
        "            if string:\n",
        "                label = \"false\"\n",
        "            else:\n",
        "                label = 0\n",
        "                \n",
        "    elif 'true' in annotation.keys() and 'misinformation' not in annotation.keys():\n",
        "        print ('Has true not misinformation')\n",
        "        label = None\n",
        "    else:\n",
        "        print('No annotations')\n",
        "        label = None\n",
        "           \n",
        "    return label\n",
        "#load veracity labels\n",
        "veracity_dict={}\n",
        "root_dir='/content/drive/My Drive/PHEME'\n",
        "for e in event_labels:\n",
        "  dirs=os.listdir(root_dir + '/{}'.format(e))\n",
        "  for d in dirs:\n",
        "    if(d=='.DS_Store'):\n",
        "      continue\n",
        "    with open(root_dir+'/'+e+'/'+d+ '/annotation.json','r') as f:\n",
        "      veracity_dict.update({d:veracity_enum[convert_annotations(json.load(f))]})\n",
        "\n",
        "# load stance labels\n",
        "stance_dict={}\n",
        "stance_annotation_file=root_dir+'/annotations/en-scheme-annotations.json'\n",
        "with open(stance_annotation_file,'r') as f:\n",
        "  lines=f.readlines()\n",
        "for line in lines:\n",
        "  if('#' in line):\n",
        "    continue\n",
        "  annot=ast.literal_eval(line)\n",
        "  if('responsetype-vs-source' in annot.keys()):\n",
        "    stance_dict[annot['tweetid']]=stance_enum[annot['responsetype-vs-source']]\n",
        "  elif('responsetype-vs-previous' in annot.keys()):\n",
        "    stance_dict[annot['tweetid']]=stance_enum[annot['responsetype-vs-previous']]\n",
        "\n",
        "# veracity_dict={}\n",
        "# for file in rumour_files:\n",
        "#   with open(file,'r') as f:\n",
        "#     veracity_dict.update(json.load(f,parse_int=None))\n",
        "# for k,v in veracity_dict.items():\n",
        "#   veracity_dict[k]=veracity_enum[v]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6MfputFM0Ez",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9f2e40d4-b004-42ed-e3a6-d72ea0a6cd1d"
      },
      "source": [
        "embedding=BERT()\n",
        "with open('/content/drive/My Drive/rumoureval-data/charliehebdo/553184482241814530/replies/553184702530453505.json','r') as f:\n",
        "  source=json.load(f,parse_int=None)\n",
        "source={'id':source['id'],'text':embedding.embed(preprocess(source['text']))} \n",
        "source\n",
        "# type(encoder.encode(['maybe', 'they', 'want', 'to', 'get' ,'caught']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils.py:831: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.\n",
            "  category=FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 553184702530453505,\n",
              " 'text': tensor([[-0.3701, -0.2217,  0.9532,  0.5189,  0.8301,  0.2629,  0.8415,  0.6777,\n",
              "           0.9937, -0.9796,  0.9742, -0.8071,  0.9026, -0.2117,  0.9733, -0.5808,\n",
              "           0.0143, -0.3522,  0.3882, -0.8300,  0.7825,  0.0173,  0.8494,  0.6325,\n",
              "           0.3077, -0.8371, -0.4423,  0.9895,  0.9898,  0.4993, -0.1705, -0.5002,\n",
              "          -0.8806,  0.4054,  0.8998, -0.1324, -0.6142, -0.3484, -0.4314,  0.5374,\n",
              "          -0.9941,  0.6816,  0.7429, -0.9557,  0.6775,  0.0691, -0.0355, -0.3049,\n",
              "          -0.6994, -0.9876, -0.4933, -0.8455, -0.5568,  0.0603, -0.3631, -0.1101,\n",
              "          -0.6004, -0.7368, -0.5735,  0.1829,  0.0956, -0.5329,  0.7888, -0.8245,\n",
              "          -0.9179, -0.9017, -0.1838, -0.5882, -0.3484, -0.6698,  0.4890, -0.0192,\n",
              "           0.2107, -0.9385, -0.8362, -0.6202, -0.2919,  0.2919,  0.1369, -0.9912,\n",
              "          -0.9393, -0.8425, -0.2211,  0.8951, -0.6955,  0.0378, -0.8312,  0.2405,\n",
              "          -0.9967,  0.0448,  0.6354,  0.3575, -0.5427,  0.5747,  0.6675, -0.1693,\n",
              "           0.0197,  0.5401, -0.4984,  0.1811,  0.1552, -0.5009, -0.1438,  0.1869,\n",
              "           0.6898, -0.2626, -0.1717,  0.2362, -0.0249, -0.1254,  0.0480, -0.4671,\n",
              "          -0.1886, -0.7974, -0.4206,  0.5028, -0.9134,  0.3423, -0.9877,  0.0727,\n",
              "          -0.0406, -0.6431,  0.9892,  0.7438, -0.4995, -0.1511,  0.9927, -0.8214,\n",
              "           0.8687, -0.0556, -0.2533,  0.6068, -0.9716, -0.9809,  0.0788,  0.9862,\n",
              "          -0.3816,  0.8257, -0.3494,  0.9726,  0.8980,  0.9339, -0.8401,  0.2302,\n",
              "          -0.3241,  0.6796, -0.4616, -0.3014, -0.4131, -0.5775, -0.1803,  0.4278,\n",
              "           0.5129, -0.6538, -0.2762,  0.9907,  0.8497,  0.8123,  0.9724,  0.0573,\n",
              "          -0.6128,  0.7275, -0.0093,  0.5444, -0.5326,  0.1199, -0.4755, -0.5354,\n",
              "          -0.8735,  0.4846, -0.6161,  0.4237,  0.9326, -0.9949,  0.1759, -0.0439,\n",
              "           0.9921,  0.4536,  0.3807, -0.9298,  0.2727, -0.6183, -0.9875,  0.9714,\n",
              "          -0.4142, -0.5929, -0.6860, -0.1327, -0.9855, -0.0360, -0.2293,  0.5054,\n",
              "          -0.9814, -0.1369, -0.7137,  0.4390, -0.0664,  0.4965,  0.2441,  0.4117,\n",
              "           0.0263,  0.9980,  0.8206,  0.9027, -0.6468, -0.3879, -0.8211, -0.6490,\n",
              "          -0.5713,  0.3311,  0.1516,  0.9896, -0.6979, -0.2119, -0.9383, -0.9906,\n",
              "          -0.2072, -0.9760, -0.5267,  0.0889, -0.1775, -0.7618, -0.8560, -0.0200,\n",
              "          -0.1203, -0.9134, -0.2460,  0.4373, -0.0912,  0.5182,  0.9027, -0.9796,\n",
              "          -0.7560,  0.1069,  0.9924,  0.7499, -0.7314,  0.8372,  0.1689,  0.5415,\n",
              "           0.6279,  0.8091, -0.6898, -0.7976, -0.9352,  0.8243, -0.3257,  0.9765,\n",
              "          -0.4589, -0.2390, -0.9914,  0.4131, -0.0486, -0.0667,  0.1900,  0.8301,\n",
              "          -0.9526, -0.9405, -0.9887,  0.7565, -0.9364, -0.8312, -0.3670,  0.1302,\n",
              "           0.1288,  0.0215, -0.9761,  0.8076, -0.3667,  0.8099, -0.8587,  0.0078,\n",
              "           0.8507, -0.9257, -0.4570, -0.4075,  0.9811, -0.1859, -0.9840,  0.3718,\n",
              "           0.6772,  0.6323,  0.8710,  0.1266,  0.6995,  0.9837,  0.9746, -0.3238,\n",
              "          -0.8425, -0.9805,  0.6209,  0.9793, -0.8974, -0.9386,  0.1354,  0.6677,\n",
              "          -0.7241, -0.7617,  0.1522, -0.8346, -0.9238,  0.9805,  0.9027, -0.4903,\n",
              "           0.9349,  0.9880,  0.5876, -0.9627, -0.3602,  0.7517, -0.1278,  0.6921,\n",
              "           0.1705, -0.0695,  0.8653, -0.6224,  0.9374,  0.4449, -0.0585, -0.4620,\n",
              "          -0.4764, -0.9683, -0.2873,  0.3873, -0.2449, -0.9777, -0.7162, -0.9792,\n",
              "           0.6431,  0.4720,  0.2945, -0.6793, -0.5873, -0.1299,  0.6167, -0.4566,\n",
              "          -0.3041,  0.0363,  0.7422, -0.9396,  0.8561, -0.9899,  0.9023, -0.4112,\n",
              "          -0.9962,  0.5795,  0.6884, -0.9793,  0.4698, -0.5168, -0.0734,  0.6111,\n",
              "          -0.6633, -0.9679,  0.5234,  0.1859,  0.3249,  0.1661,  0.9835,  0.0342,\n",
              "           0.8739,  0.8578,  0.9827, -0.9801, -0.6531, -0.2214, -0.9867,  0.9783,\n",
              "           0.9396,  0.6129, -0.9482, -0.5322,  0.0976, -0.3454, -0.8688, -0.7892,\n",
              "           0.6425, -0.0715,  0.9421,  0.7474,  0.5856, -0.2838,  0.5736,  0.8443,\n",
              "          -0.6040, -0.2265, -0.5514, -0.4291,  0.1629, -0.7186, -0.9606,  0.4373,\n",
              "           0.7778,  0.9319,  0.4361,  0.6424, -0.4906, -0.0037,  0.2804,  0.5839,\n",
              "           0.3474, -0.9619, -0.2229,  0.0785, -0.9723,  0.9182,  0.0640, -0.3942,\n",
              "           0.1430, -0.2042, -0.0169, -0.2917, -0.9819,  0.2123, -0.1970, -0.9089,\n",
              "           0.9891,  0.6388,  0.2586,  0.8221,  0.9740, -0.1172, -0.7596, -0.2430,\n",
              "          -0.9176, -0.1339, -0.9844,  0.9909, -0.9966,  0.4481, -0.0497, -0.9174,\n",
              "           0.5177, -0.9970,  0.8562,  0.9023, -0.0503,  0.1259, -0.9192,  0.2905,\n",
              "          -0.6035,  0.9798, -0.3864,  0.2381, -0.8487, -0.9500, -0.9068, -0.7444,\n",
              "          -0.8599,  0.9797, -0.8239, -0.5041, -0.7897,  0.0877,  0.0705, -0.9117,\n",
              "           0.3173, -0.9890,  0.9372,  0.4266,  0.6111,  0.3943, -0.1115, -0.2845,\n",
              "           0.8607,  0.0367, -0.3866,  0.0656, -0.8435,  0.0787, -0.8298,  0.2577,\n",
              "           0.4427,  0.1591,  0.5287, -0.6884,  0.7211,  0.6985,  0.5777,  0.3644,\n",
              "           0.1092,  0.8207,  0.9850,  0.9799,  0.9453,  0.4133,  0.4811, -0.0424,\n",
              "          -0.5725,  0.8857,  0.4909, -0.0143, -0.4437, -0.2643,  0.3756,  0.6279,\n",
              "           0.2081, -0.3845, -0.5603,  0.4393, -0.2455,  0.7026, -0.2765,  0.2220,\n",
              "          -0.9130,  0.5588, -0.4635,  0.8799,  0.5819, -0.5412, -0.3469, -0.9103,\n",
              "           0.5407,  0.5059,  0.1720,  0.1237, -0.5829, -0.0347,  0.9410, -0.0186,\n",
              "          -0.9976, -0.8865,  0.3744, -0.9896, -0.4779, -0.2492,  0.5234, -0.6409,\n",
              "          -0.9078, -0.1924,  0.5682, -0.9879,  0.6669,  0.0685,  0.9776,  0.5206,\n",
              "           0.3989, -0.8661, -0.8567, -0.6512,  0.9867, -0.8815,  0.8841, -0.9777,\n",
              "          -0.0694,  0.5884,  0.1461, -0.8232,  0.0846,  0.0114, -0.2839,  0.8034,\n",
              "           0.3070, -0.9945,  0.0840, -0.3534, -0.5643, -0.4644, -0.5767,  0.8588,\n",
              "           0.3590, -0.0854, -0.4087, -0.2015, -0.4446,  0.5809,  0.3436,  0.2942,\n",
              "          -0.6731, -0.4279, -0.9619,  0.0383, -0.1081,  0.0607,  0.7771, -0.0650,\n",
              "          -0.0356, -0.9137, -0.0055,  0.8886,  0.2997, -0.7110, -0.6382,  0.8176,\n",
              "           0.9816,  0.6141, -0.0069,  0.8431, -0.1927, -0.3729, -0.1111,  0.4238,\n",
              "           0.7672,  0.7184,  0.1008,  0.1388,  0.4552, -0.3218, -0.6550, -0.5007,\n",
              "          -0.5273,  0.8131, -0.2757, -0.9864,  0.5846,  0.4063, -0.9402, -0.1307,\n",
              "           0.5326, -0.6407,  0.8055,  0.9947,  0.4126,  0.5354, -0.1860, -0.3885,\n",
              "          -0.1079, -0.5918, -0.9700,  0.9461,  0.3205,  0.5256, -0.4608,  0.2529,\n",
              "           0.9622,  0.6217, -0.3511, -0.4733,  0.2403, -0.1477, -0.9408,  0.8746,\n",
              "          -0.9903, -0.1325, -0.9425, -0.3779,  0.5365,  0.8895,  0.2612,  0.9375,\n",
              "           0.9497, -0.7520,  0.9735,  0.9107, -0.5896, -0.9685, -0.9808, -0.9037,\n",
              "          -0.7850,  0.2021, -0.7522,  0.7151, -0.1862, -0.6591, -0.2312, -0.1900,\n",
              "           0.8080,  0.1589, -0.8655,  0.9689,  0.1508, -0.8049,  0.5811, -0.9930,\n",
              "          -0.9056,  0.1106,  0.0075,  0.5245, -0.7098,  0.9691,  0.1486, -0.0148,\n",
              "          -0.6482,  0.7375, -0.9898, -0.8893,  0.5745,  0.8654, -0.7432,  0.9733,\n",
              "          -0.8189,  0.2740,  0.8865,  0.2413,  0.5710,  0.5056, -0.2893, -0.4843,\n",
              "           0.7339,  0.6759,  0.8010,  0.8836,  0.8291, -0.3986,  0.9564, -0.0691,\n",
              "           0.9880, -0.9734, -0.4166,  0.7420, -0.4612, -0.1601,  0.0043, -0.8368,\n",
              "           0.3838,  0.5687,  0.7586,  0.1462,  0.3340,  0.3970,  0.5892,  0.0107,\n",
              "          -0.3000,  0.0547,  0.4464,  0.9906,  0.6816, -0.1262,  0.7800,  0.2885,\n",
              "           0.6382, -0.5812,  0.4866, -0.4239,  0.1953,  0.3725, -0.3428,  0.9797,\n",
              "          -0.9098,  0.2932, -0.4607, -0.3519, -0.5382, -0.8344, -0.4437, -0.3525,\n",
              "           0.0907,  0.3019,  0.7013,  0.7060,  0.9591, -0.3640, -0.5760,  0.5018,\n",
              "           0.3969, -0.4441, -0.7112,  0.7229, -0.7255,  0.9638, -0.7761,  0.9556,\n",
              "          -0.9768,  0.5049,  0.9448, -0.7767,  0.2315,  0.6336, -0.7288,  0.9751,\n",
              "          -0.8467,  0.8856, -0.7716,  0.5763, -0.5588, -0.4134, -0.6316,  0.9730]],\n",
              "        grad_fn=<TanhBackward>)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy9VIIV4xg74",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "9fd26d78-e2e5-45de-ca4a-2e61ae113915"
      },
      "source": [
        "import pickle\n",
        "# import importlib\n",
        "# importlib.reload(examples.encode_texts)\n",
        "\n",
        "def dump_embedding(typ):\n",
        "  root_path='/content/drive/My Drive/PHEME/'\n",
        "  for e in ['charliehebdo','sydneysiege','germanwings-crash','ferguson','ottawashooting']:\n",
        "  # for e in ['putinmissing','ebola-essien','prince-toronto']:\n",
        "    # event_list=[]\n",
        "    ev=Event(root_path+e)\n",
        "    ev.create_tree(gb)\n",
        "    # event_list.append(ev)\n",
        "    with open('/content/drive/My Drive/PHEME/Encoding/{}/data_{}.pkl'.format(typ,e),'wb') as f:\n",
        "      pickle.dump(ev,f)\n",
        "\n",
        "# embedding=SKP(encoder)\n",
        "# embedding=DeepMoji()\n",
        "embedding=BERT()\n",
        "gb=Global()\n",
        "# dump_embedding('skp')\n",
        "dump_embedding('BERT')      "
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils.py:831: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.\n",
            "  category=FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_T44TQyY2D_O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[(node.id,node.x) for node in gb.g_nodes]\n",
        "# with open('/content/drive/My Drive/data.pkl','rb') as f:\n",
        "#   event_list=pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-49_yrZInieX",
        "colab_type": "text"
      },
      "source": [
        "# Veracity Tree Balancing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqWWZHbLu4Pc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "728cb1db-4a53-44d6-98c7-81659e9b87e0"
      },
      "source": [
        "# Balancing each class for rumour labels\n",
        "def balance_embedding(typ):\n",
        "  ev_label_count=[]\n",
        "  for e in ['charliehebdo','sydneysiege','germanwings-crash','ferguson','ottawashooting']:\n",
        "    with open('/content/drive/My Drive/PHEME/Encoding/{}/data_{}.pkl'.format(typ,e),'rb') as f:\n",
        "      ev=pickle.load(f)\n",
        "    label={0:0,1:0,2:0}   \n",
        "    # for event in ev:\n",
        "    for tree in ev.treelist:\n",
        "      label[veracity_dict[str(tree[0])]]+=1\n",
        "    ev_label_count.append((e,label))    \n",
        "  print(ev_label_count)\n",
        "  for e in ['charliehebdo','sydneysiege','germanwings-crash','ferguson','ottawashooting']:\n",
        "    with open('/content/drive/My Drive/PHEME/Encoding/{}/data_{}.pkl'.format(typ,e),'rb') as f:\n",
        "      ev=pickle.load(f) \n",
        "    # for event in ev:\n",
        "    lbl_0_list=[]\n",
        "    lbl_1_list=[]\n",
        "    lbl_2_list=[]\n",
        "    for tree in ev.treelist: \n",
        "      if(veracity_dict[str(tree[0])]==0):\n",
        "        lbl_0_list.append(tree)      \n",
        "      elif(veracity_dict[str(tree[0])]==1):\n",
        "        lbl_1_list.append(tree)\n",
        "      elif(veracity_dict[str(tree[0])]==2):\n",
        "        lbl_2_list.append(tree)\n",
        "    if(e=='charliehebdo'):\n",
        "      for i in range(27):\n",
        "        ev.treelist.append(random.sample(lbl_1_list,1)[0])\n",
        "      for i in range(16):\n",
        "        ev.treelist.append(random.sample(lbl_2_list,1)[0])\n",
        "    elif(e=='sydneysiege'):\n",
        "      for i in range(38):\n",
        "        ev.treelist.append(random.sample(lbl_1_list,1)[0])\n",
        "      for i in range(44):\n",
        "        ev.treelist.append(random.sample(lbl_2_list,1)[0])   \n",
        "    elif(e=='germanwings-crash'):\n",
        "      for i in range(9):\n",
        "        ev.treelist.append(random.sample(lbl_2_list,1)[0]) \n",
        "    elif(e=='ferguson'):\n",
        "      for i in range(42):\n",
        "        ev.treelist.append(random.sample(lbl_0_list,1)[0])\n",
        "    elif(e=='ottawashooting'):\n",
        "      for i in range(40):\n",
        "        ev.treelist.append(random.sample(lbl_1_list,1)[0])    \n",
        "      for i in range(23):\n",
        "        ev.treelist.append(random.sample(lbl_2_list,1)[0])    \n",
        "    with open('/content/drive/My Drive/PHEME/Encoding/{}/data_{}_balanced.pkl'.format(typ,e),'wb') as f:\n",
        "      pickle.dump(ev,f) \n",
        "\n",
        "  ev_label_count=[]\n",
        "  for e in ['charliehebdo','sydneysiege','germanwings-crash','ferguson','ottawashooting']:\n",
        "    with open('/content/drive/My Drive/PHEME/Encoding/{}/data_{}_balanced.pkl'.format(typ,e),'rb') as f:\n",
        "      ev=pickle.load(f)\n",
        "    label={0:0,1:0,2:0}   \n",
        "    # for event in ev:\n",
        "    for tree in ev.treelist:\n",
        "      label[veracity_dict[str(tree[0])]]+=1\n",
        "    ev_label_count.append((e,label))    \n",
        "  print(ev_label_count) \n",
        "\n",
        "# balance_embedding('skp')                                                                   \n",
        "# balance_embedding('deepmoji')\n",
        "balance_embedding('BERT')                                                                   "
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('charliehebdo', {0: 39, 1: 12, 2: 23}), ('sydneysiege', {0: 51, 1: 13, 2: 7}), ('germanwings-crash', {0: 10, 1: 12, 2: 3}), ('ferguson', {0: 2, 1: 0, 2: 44}), ('ottawashooting', {0: 35, 1: 10, 2: 13})]\n",
            "[('charliehebdo', {0: 39, 1: 39, 2: 39}), ('sydneysiege', {0: 51, 1: 51, 2: 51}), ('germanwings-crash', {0: 10, 1: 12, 2: 12}), ('ferguson', {0: 44, 1: 0, 2: 44}), ('ottawashooting', {0: 35, 1: 50, 2: 36})]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgnvlbS4nrt9",
        "colab_type": "text"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8DIn9YRb7wV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "event_labels=['charliehebdo','sydneysiege','germanwings-crash','ferguson','ottawashooting']\n",
        "# change type for embedding skp,deepmoji,bert,mix(deepmoji+skp)\n",
        "def train_loader(val_index):\n",
        "  for e in [l for i,l in enumerate(event_labels) if i!=val_index]:\n",
        "    with open('/content/drive/My Drive/PHEME/Encoding/BERT/data_{}_balanced.pkl'.format(e),'rb') as f:\n",
        "      ev=pickle.load(f)\n",
        "    yield ev,e\n",
        "\n",
        "def val_loader(val_index):\n",
        "  for e in [l for i,l in enumerate(event_labels) if i==val_index]:\n",
        "    with open('/content/drive/My Drive/PHEME/Encoding/BERT/data_{}.pkl'.format(e),'rb') as f:\n",
        "      ev=pickle.load(f)\n",
        "    yield ev,e"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYHYx66fEFpq",
        "colab_type": "text"
      },
      "source": [
        "# Training without stance loss averaging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl308Al3nxB2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "outputId": "f483948b-a965-4854-8a95-00cf8dcd13cd"
      },
      "source": [
        "# Training on unbalanced data without loss averaging for stance\n",
        "model=TreeLSTMCell(768,768)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=.008)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "total_loss=[]\n",
        "for e in range(5):  \n",
        "  print('==================== Iteration {} =========='.format(e))\n",
        "  val_index=e%5\n",
        "  train_loss=0\n",
        "  for event,lbl in train_loader(val_index):\n",
        "    print('training on {}...'.format(lbl))    \n",
        "    for tree in event.treelist:\n",
        "      loss=0\n",
        "      stance_loss=0\n",
        "      optimizer.zero_grad()  \n",
        "      nodes=tree[1]\n",
        "      for i in range(len(nodes)):\n",
        "        nodes[i].h=None\n",
        "        nodes[i].c=None\n",
        "      model.reset_h_c()  \n",
        "      for i in range(len(nodes)):\n",
        "        if(i!=(len(nodes)-1)):\n",
        "          result=model(nodes[i])\n",
        "          if(result is not None and nodes[i].y is not None):\n",
        "            stance_loss+=criterion(result,torch.tensor(nodes[i].y).view(-1))            \n",
        "        else:\n",
        "          result=model(nodes[i],is_last=True)       \n",
        "          loss+=criterion(result,torch.tensor(veracity_dict[str(tree[0])],dtype=torch.long).view(-1))   \n",
        "      loss+=stance_loss            \n",
        "      train_loss+=loss.data.item()        \n",
        "      loss.backward()\n",
        "      optimizer.step()    \n",
        "    print('train loss : {}'.format(train_loss))     \n",
        "  with torch.no_grad():\n",
        "    validation_loss=0\n",
        "    for event,lbl in val_loader(val_index):\n",
        "      print('testing on {}...'.format(lbl))    \n",
        "      for tree in event.treelist:\n",
        "        loss=0\n",
        "        nodes=tree[1]\n",
        "        for i in range(len(nodes)):\n",
        "          nodes[i].h=None\n",
        "          nodes[i].c=None\n",
        "        model.reset_h_c()  \n",
        "        for i in range(len(nodes)):\n",
        "          if(i!=(len(nodes)-1)):\n",
        "            result=model(nodes[i])\n",
        "            if(result is not None and nodes[i].y is not None):\n",
        "              stance_loss+=criterion(result,torch.tensor(nodes[i].y).view(-1))\n",
        "          else:\n",
        "            result=model(nodes[i],is_last=True)      \n",
        "            loss+=criterion(result,torch.tensor(veracity_dict[str(tree[0])],dtype=torch.long).view(-1))  \n",
        "        loss+=stance_loss            \n",
        "        validation_loss+=loss.data.item()   \n",
        "      print('validation loss : {}'.format(validation_loss))\n",
        "      torch.save(model.state_dict(), '/content/drive/My Drive/model_bert_balanced_30.pt')    "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==================== Iteration 0 ==========\n",
            "training on sydneysiege...\n",
            "train loss : 2890.2772336006165\n",
            "training on germanwings-crash...\n",
            "train loss : 3314.25519824028\n",
            "training on ferguson...\n",
            "train loss : 5115.741343259811\n",
            "training on ottawashooting...\n",
            "train loss : 6866.604239463806\n",
            "testing on charliehebdo...\n",
            "validation loss : 38604.97555541992\n",
            "==================== Iteration 1 ==========\n",
            "training on charliehebdo...\n",
            "train loss : 1539.7985394001007\n",
            "training on germanwings-crash...\n",
            "train loss : 1934.5494248867035\n",
            "training on ferguson...\n",
            "train loss : 3711.055954694748\n",
            "training on ottawashooting...\n",
            "train loss : 5434.356980085373\n",
            "testing on sydneysiege...\n",
            "validation loss : 43696.31030654907\n",
            "==================== Iteration 2 ==========\n",
            "training on charliehebdo...\n",
            "train loss : 1532.8338586091995\n",
            "training on sydneysiege...\n",
            "train loss : 4093.521251797676\n",
            "training on ferguson...\n",
            "train loss : 5890.21211540699\n",
            "training on ottawashooting...\n",
            "train loss : 7619.407017469406\n",
            "testing on germanwings-crash...\n",
            "validation loss : 3672.981565475464\n",
            "==================== Iteration 3 ==========\n",
            "training on charliehebdo...\n",
            "train loss : 1501.06569314003\n",
            "training on sydneysiege...\n",
            "train loss : 4093.758560925722\n",
            "training on germanwings-crash...\n",
            "train loss : 4570.383615642786\n",
            "training on ottawashooting...\n",
            "train loss : 6389.427087932825\n",
            "testing on ferguson...\n",
            "validation loss : 25003.227556228638\n",
            "==================== Iteration 4 ==========\n",
            "training on charliehebdo...\n",
            "train loss : 1571.9420629739761\n",
            "training on sydneysiege...\n",
            "train loss : 4232.188250303268\n",
            "training on germanwings-crash...\n",
            "train loss : 4716.17014837265\n",
            "training on ferguson...\n",
            "train loss : 6632.974595546722\n",
            "testing on ottawashooting...\n",
            "validation loss : 21220.884895324707\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPx34ziV0uDg",
        "colab_type": "text"
      },
      "source": [
        "# Training with stance loss averaging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a33LzQVD-Ev",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "outputId": "ea93fd31-690c-43d7-94d9-9edb1572f1a1"
      },
      "source": [
        "# skp:\n",
        "# model=TreeLSTMCell(2400,2400)\n",
        "# deepmoji\n",
        "model=TreeLSTMCell(768,768)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# validation_losses={'charliehebdo':0,'sydneysiege':0,'germanwings-crash':0,'ferguson':0,'ottawashooting':0}\n",
        "total_loss=[]\n",
        "for e in range(5):  \n",
        "  print('==================== Iteration {} =========='.format(e))\n",
        "  val_index=e%5\n",
        "  train_loss=0\n",
        "  for event,lbl in train_loader(val_index):\n",
        "    print('training on {}...'.format(lbl))    \n",
        "    for tree in event.treelist:\n",
        "      loss=0\n",
        "      stance_loss=0\n",
        "      optimizer.zero_grad()      \n",
        "      stance_loss_0=[]\n",
        "      stance_loss_1=[]\n",
        "      stance_loss_2=[]\n",
        "      stance_loss_3=[]\n",
        "      nodes=tree[1]\n",
        "      for i in range(len(nodes)):\n",
        "        nodes[i].h=None\n",
        "        nodes[i].c=None\n",
        "      model.reset_h_c()  \n",
        "      for i in range(len(nodes)):\n",
        "        if(i!=(len(nodes)-1)):\n",
        "          result=model(nodes[i])\n",
        "          if(result is not None and nodes[i].y is not None):\n",
        "            if(e>28):\n",
        "              print('stance : {}'.format(result),nodes[i].y)\n",
        "            if(nodes[i].y==0):\n",
        "              stance_loss_0.append(criterion(result,torch.tensor(nodes[i].y).view(-1)))\n",
        "            if(nodes[i].y==1):\n",
        "              stance_loss_1.append(criterion(result,torch.tensor(nodes[i].y).view(-1)))\n",
        "            if(nodes[i].y==2):\n",
        "              stance_loss_2.append(criterion(result,torch.tensor(nodes[i].y).view(-1)))\n",
        "            if(nodes[i].y==3):\n",
        "              stance_loss_3.append(criterion(result,torch.tensor(nodes[i].y).view(-1)))\n",
        "        else:\n",
        "          result=model(nodes[i],is_last=True)  \n",
        "          # print(result,veracity_dict[str(tree[0])],nodes[i].id)        \n",
        "          loss+=criterion(result,torch.tensor(veracity_dict[str(tree[0])],dtype=torch.long).view(-1))  \n",
        "      if(len(stance_loss_0)>0):\n",
        "        stance_loss_0=sum(stance_loss_0)/len(stance_loss_0)\n",
        "      else:\n",
        "        stance_loss_0=0   \n",
        "      if(len(stance_loss_1)>0):\n",
        "        stance_loss_1=sum(stance_loss_1)/len(stance_loss_1)\n",
        "      else:\n",
        "        stance_loss_1=0        \n",
        "      if(len(stance_loss_2)>0):\n",
        "        stance_loss_2=sum(stance_loss_2)/len(stance_loss_2)\n",
        "      else:\n",
        "        stance_loss_2=0        \n",
        "      if(len(stance_loss_3)>0):\n",
        "        stance_loss_3=sum(stance_loss_3)/len(stance_loss_3)\n",
        "      else:\n",
        "        stance_loss_3=0   \n",
        "      # if(e>28):      \n",
        "      #   print('stance0 loss: {}'.format(stance_loss_0),'stance1 loss: {}'.format(stance_loss_1),\\\n",
        "      #         'stance2 loss: {}'.format(stance_loss_2),'stance3 loss: {}'.format(stance_loss_3))\n",
        "      stance_loss+=stance_loss_0+stance_loss_1+stance_loss_2+stance_loss_3\n",
        "    # if(e>28):\n",
        "    #   print('stance_loss:{}'.format(stance_loss))\n",
        "      loss+=stance_loss            \n",
        "      train_loss+=loss.data.item()        \n",
        "      loss.backward()\n",
        "      optimizer.step()    \n",
        "    print('train loss : {}'.format(train_loss))     \n",
        "  with torch.no_grad():\n",
        "    validation_loss=0\n",
        "    for event,lbl in val_loader(val_index):\n",
        "      print('testing on {}...'.format(lbl))    \n",
        "      for tree in event.treelist:\n",
        "        loss=0\n",
        "        stance_loss=0        \n",
        "        stance_loss_0=[]\n",
        "        stance_loss_1=[]\n",
        "        stance_loss_2=[]\n",
        "        stance_loss_3=[]\n",
        "        nodes=tree[1]\n",
        "        for i in range(len(nodes)):\n",
        "          nodes[i].h=None\n",
        "          nodes[i].c=None\n",
        "        model.reset_h_c()  \n",
        "        for i in range(len(nodes)):\n",
        "          if(i!=(len(nodes)-1)):\n",
        "            result=model(nodes[i])\n",
        "            if(result is not None and nodes[i].y is not None):\n",
        "              # if(e>28):\n",
        "              #   print('stance : {}'.format(result),nodes[i].y)\n",
        "              if(nodes[i].y==0):\n",
        "                stance_loss_0.append(criterion(result,torch.tensor(nodes[i].y).view(-1)))\n",
        "              if(nodes[i].y==1):\n",
        "                stance_loss_1.append(criterion(result,torch.tensor(nodes[i].y).view(-1)))\n",
        "              if(nodes[i].y==2):\n",
        "                stance_loss_2.append(criterion(result,torch.tensor(nodes[i].y).view(-1)))\n",
        "              if(nodes[i].y==3):\n",
        "                stance_loss_3.append(criterion(result,torch.tensor(nodes[i].y).view(-1)))\n",
        "          else:\n",
        "            result=model(nodes[i],is_last=True)  \n",
        "            # print(result,veracity_dict[str(tree[0])],nodes[i].id)        \n",
        "            loss+=criterion(result,torch.tensor(veracity_dict[str(tree[0])],dtype=torch.long).view(-1))  \n",
        "        if(len(stance_loss_0)>0):\n",
        "          stance_loss_0=sum(stance_loss_0)/len(stance_loss_0)\n",
        "        else:\n",
        "          stance_loss_0=0   \n",
        "        if(len(stance_loss_1)>0):\n",
        "          stance_loss_1=sum(stance_loss_1)/len(stance_loss_1)\n",
        "        else:\n",
        "          stance_loss_1=0        \n",
        "        if(len(stance_loss_2)>0):\n",
        "          stance_loss_2=sum(stance_loss_2)/len(stance_loss_2)\n",
        "        else:\n",
        "          stance_loss_2=0        \n",
        "        if(len(stance_loss_3)>0):\n",
        "          stance_loss_3=sum(stance_loss_3)/len(stance_loss_3)\n",
        "        else:\n",
        "          stance_loss_3=0   \n",
        "        # if(e>28):      \n",
        "        #   print('stance0 loss: {}'.format(stance_loss_0),'stance1 loss: {}'.format(stance_loss_1),\\\n",
        "        #         'stance2 loss: {}'.format(stance_loss_2),'stance3 loss: {}'.format(stance_loss_3))\n",
        "        stance_loss+=stance_loss_0+stance_loss_1+stance_loss_2+stance_loss_3\n",
        "      # if(e>28):\n",
        "      #   print('stance_loss:{}'.format(stance_loss))\n",
        "        loss+=stance_loss            \n",
        "        validation_loss+=loss.data.item()   \n",
        "      print('validation loss : {}'.format(validation_loss))\n",
        "      \n",
        "    # if(e>0 and e%5==0): \n",
        "    #   sum_loss=sum([v for k,v in validation_loss])\n",
        "      # if(e/5>0 and validation_loss<validation_losses[lbl]):\n",
        "      torch.save(model.state_dict(), '/content/drive/My Drive/model_bert_balanced_stance_averaged.pt')\n",
        "      # else:\n",
        "        # exit()        \n",
        "      # validation_losses[lbl]=validation_loss         \n",
        "      # total_loss.append(sum_loss)      "
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==================== Iteration 0 ==========\n",
            "training on sydneysiege...\n",
            "train loss : 754.097038269043\n",
            "training on germanwings-crash...\n",
            "train loss : 955.8442323207855\n",
            "training on ferguson...\n",
            "train loss : 1415.5786092281342\n",
            "training on ottawashooting...\n",
            "train loss : 1994.668744623661\n",
            "testing on charliehebdo...\n",
            "validation loss : 591.5271751880646\n",
            "==================== Iteration 1 ==========\n",
            "training on charliehebdo...\n",
            "train loss : 538.2599716186523\n",
            "training on germanwings-crash...\n",
            "train loss : 688.7457664012909\n",
            "training on ferguson...\n",
            "train loss : 1161.300347685814\n",
            "training on ottawashooting...\n",
            "train loss : 1716.704894900322\n",
            "testing on sydneysiege...\n",
            "validation loss : 895.4520432949066\n",
            "==================== Iteration 2 ==========\n",
            "training on charliehebdo...\n",
            "train loss : 526.8075971603394\n",
            "training on sydneysiege...\n",
            "train loss : 1295.2107627391815\n",
            "training on ferguson...\n",
            "train loss : 1729.2560676336288\n",
            "training on ottawashooting...\n",
            "train loss : 2275.6516404747963\n",
            "testing on germanwings-crash...\n",
            "validation loss : 161.31396222114563\n",
            "==================== Iteration 3 ==========\n",
            "training on charliehebdo...\n",
            "train loss : 512.5435446500778\n",
            "training on sydneysiege...\n",
            "train loss : 1241.7795563936234\n",
            "training on germanwings-crash...\n",
            "train loss : 1402.718094944954\n",
            "training on ottawashooting...\n",
            "train loss : 1937.8313482999802\n",
            "testing on ferguson...\n",
            "validation loss : 567.6656336784363\n",
            "==================== Iteration 4 ==========\n",
            "training on charliehebdo...\n",
            "train loss : 494.4580183029175\n",
            "training on sydneysiege...\n",
            "train loss : 1197.8414137363434\n",
            "training on germanwings-crash...\n",
            "train loss : 1358.8884913921356\n",
            "training on ferguson...\n",
            "train loss : 1795.1880033016205\n",
            "testing on ottawashooting...\n",
            "validation loss : 722.4511208534241\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdL7dbYYEJ_D",
        "colab_type": "text"
      },
      "source": [
        "# Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63-8Mk1qwni4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "5c945b85-a6ae-46cf-f1a9-3bea2d256100"
      },
      "source": [
        "def val_loader(val_index):\n",
        "  for e in [l for i,l in enumerate(event_labels) if i==val_index]:\n",
        "    with open('/content/drive/My Drive/PHEME/Encoding/BERT/data_{}.pkl'.format(e),'rb') as f:\n",
        "      ev=pickle.load(f)\n",
        "    yield ev,e\n",
        "\n",
        "model=TreeLSTMCell(768,768)\n",
        "model.load_state_dict(torch.load('/content/drive/My Drive/model_bert_balanced_30.pt'))\n",
        "results={'stance':{'charliehebdo':[],'sydneysiege':[],\\\n",
        "                    'germanwings-crash':[],'ferguson':[],'ottawashooting':[]}, \\\n",
        "                    'veracity':{'charliehebdo':[],'sydneysiege':[],\\\n",
        "                    'germanwings-crash':[],'ferguson':[],'ottawashooting':[]}}\n",
        "with torch.no_grad():\n",
        "  for val_index in range(5):\n",
        "    for event,lbl in val_loader(val_index):\n",
        "      print('testing on {}...'.format(lbl))    \n",
        "      for tree in event.treelist:\n",
        "        nodes=tree[1]\n",
        "        for i in range(len(nodes)):\n",
        "          nodes[i].h=None\n",
        "          nodes[i].c=None\n",
        "        model.reset_h_c()  \n",
        "        for i in range(len(nodes)):\n",
        "          if(i!=(len(nodes)-1)):\n",
        "            result=model(nodes[i])\n",
        "            if(result is not None and nodes[i].y is not None):\n",
        "              results['stance'][lbl].append((nodes[i].y,torch.argmax(result,dim=-1).item()))\n",
        "          else:\n",
        "            result=model(nodes[i],is_last=True) \n",
        "            results['veracity'][lbl].append((veracity_dict[str(tree[0])],torch.argmax(result,dim=-1).item()))              "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "testing on charliehebdo...\n",
            "testing on sydneysiege...\n",
            "testing on germanwings-crash...\n",
            "testing on ferguson...\n",
            "testing on ottawashooting...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKJiYKqGY6VW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b0f4d5dd-b22c-4975-dd06-b47974dc0dc0"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "for lbl in ['charliehebdo','sydneysiege','germanwings-crash','ferguson','ottawashooting']:\n",
        "  y_true=[y[0] for y in results['veracity'][lbl]]\n",
        "  y_pred=[y[1] for y in results['veracity'][lbl]]\n",
        "  print('=============={}==Veracity Report=============='.format(lbl))\n",
        "  print(classification_report(y_true,y_pred))\n",
        "\n",
        "  y_true=[y[0] for y in results['stance'][lbl]]\n",
        "  y_pred=[y[1] for y in results['stance'][lbl]]\n",
        "  print('=============={}==Stance Report=============='.format(lbl))\n",
        "  print(classification_report(y_true,y_pred)) "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==============charliehebdo==Veracity Report==============\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      1.00      0.69        39\n",
            "           1       0.00      0.00      0.00        12\n",
            "           2       0.00      0.00      0.00        23\n",
            "\n",
            "    accuracy                           0.53        74\n",
            "   macro avg       0.18      0.33      0.23        74\n",
            "weighted avg       0.28      0.53      0.36        74\n",
            "\n",
            "==============charliehebdo==Stance Report==============\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.02      0.03       172\n",
            "           1       0.72      1.00      0.84       723\n",
            "           2       0.00      0.00      0.00        53\n",
            "           3       0.00      0.00      0.00        54\n",
            "\n",
            "    accuracy                           0.72      1002\n",
            "   macro avg       0.33      0.25      0.22      1002\n",
            "weighted avg       0.62      0.72      0.61      1002\n",
            "\n",
            "==============sydneysiege==Veracity Report==============\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      1.00      0.84        51\n",
            "           1       0.00      0.00      0.00        13\n",
            "           2       0.00      0.00      0.00         7\n",
            "\n",
            "    accuracy                           0.72        71\n",
            "   macro avg       0.24      0.33      0.28        71\n",
            "weighted avg       0.52      0.72      0.60        71\n",
            "\n",
            "==============sydneysiege==Stance Report==============\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.01      0.03       155\n",
            "           1       0.68      1.00      0.81       713\n",
            "           2       1.00      0.01      0.02        99\n",
            "           3       1.00      0.01      0.02        86\n",
            "\n",
            "    accuracy                           0.68      1053\n",
            "   macro avg       0.80      0.26      0.22      1053\n",
            "weighted avg       0.71      0.68      0.56      1053\n",
            "\n",
            "==============germanwings-crash==Veracity Report==============\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      1.00      0.57        10\n",
            "           1       0.00      0.00      0.00        12\n",
            "           2       0.00      0.00      0.00         3\n",
            "\n",
            "    accuracy                           0.40        25\n",
            "   macro avg       0.13      0.33      0.19        25\n",
            "weighted avg       0.16      0.40      0.23        25\n",
            "\n",
            "==============germanwings-crash==Stance Report==============\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.04      0.08        48\n",
            "           1       0.67      1.00      0.80       174\n",
            "           2       0.00      0.00      0.00        28\n",
            "           3       0.00      0.00      0.00        11\n",
            "\n",
            "    accuracy                           0.67       261\n",
            "   macro avg       0.42      0.26      0.22       261\n",
            "weighted avg       0.63      0.67      0.55       261\n",
            "\n",
            "==============ferguson==Veracity Report==============\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.04      1.00      0.08         2\n",
            "           2       0.00      0.00      0.00        44\n",
            "\n",
            "    accuracy                           0.04        46\n",
            "   macro avg       0.02      0.50      0.04        46\n",
            "weighted avg       0.00      0.04      0.00        46\n",
            "\n",
            "==============ferguson==Stance Report==============\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.01      0.03       139\n",
            "           1       0.69      1.00      0.82       715\n",
            "           2       0.00      0.00      0.00        99\n",
            "           3       0.00      0.00      0.00        89\n",
            "\n",
            "    accuracy                           0.69      1042\n",
            "   macro avg       0.30      0.25      0.21      1042\n",
            "weighted avg       0.54      0.69      0.56      1042\n",
            "\n",
            "==============ottawashooting==Veracity Report==============\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      1.00      0.75        35\n",
            "           1       0.00      0.00      0.00        10\n",
            "           2       0.00      0.00      0.00        13\n",
            "\n",
            "    accuracy                           0.60        58\n",
            "   macro avg       0.20      0.33      0.25        58\n",
            "weighted avg       0.36      0.60      0.45        58\n",
            "\n",
            "==============ottawashooting==Stance Report==============\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.02      0.04       106\n",
            "           1       0.67      1.00      0.80       481\n",
            "           2       1.00      0.05      0.09        64\n",
            "           3       1.00      0.03      0.05        73\n",
            "\n",
            "    accuracy                           0.67       724\n",
            "   macro avg       0.79      0.27      0.25       724\n",
            "weighted avg       0.71      0.67      0.55       724\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLQaj4KLMfD3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "7362f68d-41f6-47b7-c9b3-e1b96ed63a6c"
      },
      "source": [
        "# Overall veracity confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "y_true=[]\n",
        "y_pred=[]\n",
        "for lbl in ['charliehebdo','sydneysiege','germanwings-crash','ferguson','ottawashooting']:\n",
        "  y_true.extend([y[0] for y in results['veracity'][lbl]])\n",
        "  y_pred.extend([y[1] for y in results['veracity'][lbl]])\n",
        "print('================Veracity Confucion matrix==============')\n",
        "print(confusion_matrix(y_true,y_pred,normalize='true'))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================Veracity Confucion matrix==============\n",
            "[[1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91xhOkyDF7_u",
        "colab_type": "text"
      },
      "source": [
        "# Testing on unknown data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-PyfqzxuCOH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "b62edfae-17b6-4633-dd38-dd129d64b95d"
      },
      "source": [
        "event_labels=['putinmissing','ebola-essien','prince-toronto']\n",
        "def val_loader(val_index):\n",
        "  for e in [l for i,l in enumerate(event_labels) if i==val_index]:\n",
        "    with open('/content/drive/My Drive/DeepMoji/data_bert_{}.pkl'.format(e),'rb') as f:\n",
        "      ev=pickle.load(f)\n",
        "    yield ev,e\n",
        "\n",
        "model=TreeLSTMCell(768,768)\n",
        "model.load_state_dict(torch.load('/content/drive/My Drive/model_bert_balanced.pt'))\n",
        "results={'stance':{'putinmissing':[],'ebola-essien':[],'prince-toronto':[]}, \\\n",
        "                    'veracity':{'putinmissing':[],'ebola-essien':[],'prince-toronto':[]}}\n",
        "with torch.no_grad():\n",
        "  for val_index in range(5):\n",
        "    for event,lbl in val_loader(val_index):\n",
        "      print('testing on {}...'.format(lbl))    \n",
        "      for tree in event.treelist:\n",
        "        nodes=tree[1]\n",
        "        for i in range(len(nodes)):\n",
        "          nodes[i].h=None\n",
        "          nodes[i].c=None\n",
        "        model.reset_h_c()  \n",
        "        for i in range(len(nodes)):\n",
        "          if(i!=(len(nodes)-1)):\n",
        "            result=model(nodes[i])\n",
        "            if(result is not None and nodes[i].y is not None):\n",
        "              results['stance'][lbl].append((nodes[i].y,torch.argmax(result,dim=-1).item()))\n",
        "          else:\n",
        "            result=model(nodes[i],is_last=True) \n",
        "            results['veracity'][lbl].append((veracity_dict[str(tree[0])],torch.argmax(result,dim=-1).item()))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "testing on putinmissing...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-29faaf47df59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'veracity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlbl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mveracity_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m: '576319832800555008'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wB-aJynvzgBP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "446ddc0b-6119-4fa0-e669-ab00edaa52ad"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "for lbl in ['putinmissing','ebola-essien','prince-toronto']:\n",
        "  y_true=[y[0] for y in results['veracity'][lbl]]\n",
        "  y_pred=[y[1] for y in results['veracity'][lbl]]\n",
        "  print('=============={}==Veracity Report=============='.format(lbl))\n",
        "  print(classification_report(y_true,y_pred))\n",
        "\n",
        "  y_true=[y[0] for y in results['stance'][lbl]]\n",
        "  y_pred=[y[1] for y in results['stance'][lbl]]\n",
        "  print('=============={}==Stance Report=============='.format(lbl))\n",
        "  print(classification_report(y_true,y_pred)) "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==============putinmissing==Veracity Report==============\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         0\n",
            "           1       0.00      0.00      0.00         1\n",
            "           2       1.00      0.25      0.40         8\n",
            "\n",
            "    accuracy                           0.22         9\n",
            "   macro avg       0.33      0.08      0.13         9\n",
            "weighted avg       0.89      0.22      0.36         9\n",
            "\n",
            "==============putinmissing==Stance Report==============\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.22      0.18      0.20        11\n",
            "           1       0.63      0.75      0.69        32\n",
            "           2       0.33      0.20      0.25         5\n",
            "           3       0.33      0.20      0.25         5\n",
            "\n",
            "    accuracy                           0.53        53\n",
            "   macro avg       0.38      0.33      0.35        53\n",
            "weighted avg       0.49      0.53      0.50        53\n",
            "\n",
            "==============ebola-essien==Veracity Report==============\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       0.0\n",
            "           1       0.00      0.00      0.00       2.0\n",
            "\n",
            "    accuracy                           0.00       2.0\n",
            "   macro avg       0.00      0.00      0.00       2.0\n",
            "weighted avg       0.00      0.00      0.00       2.0\n",
            "\n",
            "==============ebola-essien==Stance Report==============\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.22      0.50      0.31         4\n",
            "           1       0.53      0.43      0.47        21\n",
            "           2       0.00      0.00      0.00         1\n",
            "           3       0.00      0.00      0.00         6\n",
            "\n",
            "    accuracy                           0.34        32\n",
            "   macro avg       0.19      0.23      0.20        32\n",
            "weighted avg       0.38      0.34      0.35        32\n",
            "\n",
            "==============prince-toronto==Veracity Report==============\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         0\n",
            "           1       1.00      0.08      0.15        12\n",
            "           2       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.08        12\n",
            "   macro avg       0.33      0.03      0.05        12\n",
            "weighted avg       1.00      0.08      0.15        12\n",
            "\n",
            "==============prince-toronto==Stance Report==============\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.14      0.25      0.18        12\n",
            "           1       0.76      0.66      0.71        62\n",
            "           2       0.25      0.09      0.13        11\n",
            "           3       0.00      0.00      0.00         6\n",
            "\n",
            "    accuracy                           0.49        91\n",
            "   macro avg       0.29      0.25      0.25        91\n",
            "weighted avg       0.57      0.49      0.52        91\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzAboyuN0FjC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "6bbaeb1e-bb47-42ef-de37-848afa557bdf"
      },
      "source": [
        "# Overall veracity confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "y_true=[]\n",
        "y_pred=[]\n",
        "for lbl in ['putinmissing','ebola-essien','prince-toronto']:\n",
        "  y_true.extend([y[0] for y in results['stance'][lbl]])\n",
        "  y_pred.extend([y[1] for y in results['stance'][lbl]])\n",
        "print('================stance Confucion matrix==============')\n",
        "print(confusion_matrix(y_true,y_pred))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================stance Confucion matrix==============\n",
            "[[ 7 14  1  5]\n",
            " [27 74  2 12]\n",
            " [ 2 11  2  2]\n",
            " [ 4 10  2  1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uyhrv9woGBjC",
        "colab_type": "text"
      },
      "source": [
        "# Data distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4Ep7ENKjd-Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "04b51be6-723a-41b4-d56b-5de2a32375b2"
      },
      "source": [
        "ev_label_count=[]\n",
        "for e in ['charliehebdo','sydneysiege','germanwings-crash','ferguson','ottawashooting']:\n",
        "  with open('/content/drive/My Drive/DeepMoji/data_bert_{}.pkl'.format(e),'rb') as f:\n",
        "    ev=pickle.load(f)\n",
        "  label={0:0,1:0,2:0,3:0}   \n",
        "  # for event in ev:\n",
        "  for tree in ev.treelist:\n",
        "    nodes=tree[1]\n",
        "    for node in nodes:\n",
        "      if(node.y is not None):\n",
        "        label[node.y]+=1\n",
        "  ev_label_count.append((e,label))    \n",
        "print(ev_label_count)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('charliehebdo', {0: 169, 1: 719, 2: 53, 3: 56}), ('sydneysiege', {0: 150, 1: 700, 2: 98, 3: 88}), ('germanwings-crash', {0: 46, 1: 172, 2: 28, 3: 10}), ('ferguson', {0: 138, 1: 713, 2: 99, 3: 90}), ('ottawashooting', {0: 105, 1: 477, 2: 63, 3: 74})]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F_F-kbWvms1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "71d9008e-94e4-4e27-c133-c6b1db96d20f"
      },
      "source": [
        "ev_label_count=[]\n",
        "for e in ['charliehebdo','sydneysiege','germanwings-crash','ferguson','ottawashooting']:\n",
        "  with open('/content/drive/My Drive/DeepMoji/data_bert_{}_balanced.pkl'.format(e),'rb') as f:\n",
        "    ev=pickle.load(f)\n",
        "  label={0:0,1:0,2:0}   \n",
        "  # for event in ev:\n",
        "  for tree in ev.treelist:\n",
        "    label[veracity_dict[str(tree[0])]]+=1\n",
        "  ev_label_count.append((e,label))    \n",
        "print(ev_label_count)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('charliehebdo', {0: 39, 1: 39, 2: 39}), ('sydneysiege', {0: 51, 1: 51, 2: 51}), ('germanwings-crash', {0: 10, 1: 12, 2: 12}), ('ferguson', {0: 44, 1: 0, 2: 44}), ('ottawashooting', {0: 35, 1: 50, 2: 36})]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pelCJjRp601f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}